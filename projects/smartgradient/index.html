<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Smart Gradient</title>
  <link rel="stylesheet" href="style.css" />
  <style>
    code, pre {
      background-color: #f4f4f4;
      padding: 10px;
      display: block;
      border-radius: 6px;
      overflow-x: auto;
      font-family: monospace;
      font-size: 0.9em;
    }
    h1, h2, h3 {
      color: #1a237e;
    }
    .logo {
      width: 120px;
      height: auto;
      display: block;
      margin-bottom: 20px;
    }
  </style>
</head>
<body>
  <div class="container">

    <div style="text-align: center; margin-bottom: 20px;">
      <img src="../../images/smartgradient.png" alt="Smart Gradient Logo" style="display: block; margin: 0 auto; width: 100px;" />
      <h1>Smart Gradient</h1>
    </div>

    <p><strong>Smart Gradient</strong> is a technique to improve the accuracy of numerical gradient computations, especially in iterative optimization settings.</p>

    <p><em>Abstract:</em> Computing the gradient of a function provides fundamental information about its behavior. This information is essential for several applications and algorithms across various fields. One common application that requires gradients is optimization, such as stochastic gradient descent, Newton‚Äôs method, and trust-region methods. However, these methods often rely on numerical computation of gradients at every iteration, which is prone to numerical errors.</p>

    <p>We propose a simple limited-memory technique for improving the accuracy of these gradients by exploiting: (1) a coordinate transformation of the gradient, and (2) the history of previously taken descent directions. The method is implemented in both <strong>C++</strong> and as an <strong>R package</strong>, and verified empirically on benchmark test functions and real data applications.</p>

    <p>Below is an example showing how to use <code>smartGrad</code> in R.</p>

    <h2>Installation</h2>
    <pre><code>
library("devtools")
install_github("esmail-abdulfattah/Smart-Gradient", 
                subdir = "smartGrad")
    </code></pre>

    <h2>Usage Example</h2>

    <h3>Step 1: Define the Function</h3>
    <p>We use the Extended Rosenbrock function as an example:</p>
    <pre><code>
myfun <- function(x) {
  res <- 0.0
  for(i in 1:(length(x)-1))
    res <- res + 100*(x[i+1] - x[i]^2)^2 + (1-x[i])^2
  return(res)
}
    </code></pre>

    <h3>Step 2: Define a Standard Gradient Function</h3>
    <pre><code>
mygrad <- function(fun,x){
  h = 1e-3
  grad <- numeric(length(x))
  for(i in 1:length(x)){
    e = numeric(length(x))
    e[i] = 1
    grad[i] <- (fun(x+h*e) - fun(x-h*e))/(2*h)
  }
  return(grad)
}
    </code></pre>

    <h3>Step 3: Use Smart Gradient in Optimization</h3>
    <pre><code>
library("stats")
library("smartGrad")
x_dimension = 5
x_initial = rnorm(x_dimension)
result <- optim(par = x_initial, 
                fn = myfun,
                gr = makeSmart(fn = myfun, gr = mygrad),
                method = c("BFGS"))
    </code></pre>

    <h2>Links</h2>
    <ul>
      <li>üì¶ GitHub Repository: <a href="https://github.com/esmail-abdulfattah/Smart-Gradient" target="_blank">Smart-Gradient on GitHub</a></li>
      <li>üìÑ Published Paper: <a href="https://www.aimsciences.org/article/doi/10.3934/fods.2021037" target="_blank">Foundations of Data Science (FoDS)</a></li>
    </ul>

    <p><a href="../index.html">‚Üê Back to main site</a></p>
  </div>
</body>
</html>

